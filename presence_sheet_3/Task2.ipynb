{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data\n",
    "from torch_geometric.utils import from_networkit\n",
    "from torch_geometric import EdgeIndex\n",
    "from torch_geometric.data import Data\n",
    "from torch.distributions import Geometric\n",
    "from torch_cluster import random_walk\n",
    "import networkit as nk\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "def get_incoming_neighbors(edge_index, num_nodes):\n",
    "    # Create an empty list for each node\n",
    "    incoming_edges = {node: [] for node in range(num_nodes)}\n",
    "\n",
    "    # Iterate over the edge_index to populate the incoming_edges\n",
    "    for source, target in edge_index.t():\n",
    "        incoming_edges[target.item()].append(source.item())\n",
    "\n",
    "    return incoming_edges\n",
    "\n",
    "\n",
    "def calculate_out_degrees(edge_index, num_nodes):\n",
    "    # Initialize a tensor to hold the out-degree of each node\n",
    "    out_degrees = torch.zeros(num_nodes, dtype=torch.long)\n",
    "\n",
    "    # Count the occurrences of each node in the first row of edge_index\n",
    "    for source_node in edge_index[0]:\n",
    "        out_degrees[source_node] += 1\n",
    "\n",
    "    return out_degrees\n",
    "\n",
    "\n",
    "def geometric_random_walk(edge_index: EdgeIndex, num_nodes, prob_success, start_node):\n",
    "    # Sample a length for the random walk from a geometric distribution\n",
    "    geom_dist = Geometric(prob_success)\n",
    "    walk_length = max(\n",
    "        1, geom_dist.sample().round().int().item()\n",
    "    )  # Convert to Python int\n",
    "\n",
    "    # Perform the random walk\n",
    "    row, col = edge_index\n",
    "    # print(walk_length)\n",
    "    walk = random_walk(\n",
    "        row=row,\n",
    "        col=col,\n",
    "        start=torch.tensor([start_node], dtype=torch.long),\n",
    "        walk_length=walk_length,\n",
    "    )\n",
    "    assert walk.shape[0] == 1\n",
    "    return walk[0].tolist()\n",
    "\n",
    "\n",
    "class PPR:\n",
    "    def __init__(self, G: EdgeIndex, alpha, delta):\n",
    "        self.alpha = alpha\n",
    "        self.delta = delta\n",
    "        self.c = 350\n",
    "        self.beta = 1 / 6\n",
    "        self.e_rev = math.sqrt(delta)\n",
    "\n",
    "        self.g_edge_index = G\n",
    "        assert self.g_edge_index.num_cols == self.g_edge_index.num_rows\n",
    "        self.num_nodes = self.g_edge_index.num_cols\n",
    "        self.incoming_neighbors = get_incoming_neighbors(\n",
    "            self.g_edge_index, self.num_nodes\n",
    "        )\n",
    "        self.out_degrees = calculate_out_degrees(self.g_edge_index, self.num_nodes)\n",
    "\n",
    "    def fast_ppr(self, s, t):\n",
    "        t_set, f_set, pi_inv = self.frontier(t)\n",
    "\n",
    "        if s in t_set:\n",
    "            return pi_inv[s]\n",
    "\n",
    "        target_num_walks = int(self.c * self.e_rev / self.delta)\n",
    "        # print(target_num_walks)\n",
    "        hit_nodes = []\n",
    "        for _ in range(target_num_walks):\n",
    "            walk = geometric_random_walk(\n",
    "                self.g_edge_index, self.num_nodes, self.alpha, s\n",
    "            )\n",
    "            for n in walk:\n",
    "                if n in f_set:\n",
    "                    hit_nodes.append(n)\n",
    "                    break\n",
    "\n",
    "        result = torch.Tensor([0.0])\n",
    "        for n in hit_nodes:\n",
    "            result += pi_inv[n]\n",
    "        result /= target_num_walks\n",
    "\n",
    "        return result.item(), len(t_set) + len(f_set)\n",
    "        ...\n",
    "\n",
    "    def frontier(self, t):\n",
    "        e_inv = self.beta * self.e_rev\n",
    "\n",
    "        estimate_vector = torch.zeros([self.num_nodes])\n",
    "        estimate_vector[t] = self.alpha\n",
    "        residual_vector = estimate_vector.clone()\n",
    "\n",
    "        target_set = {t}\n",
    "        frontier_set = set(self.incoming_neighbors[t])\n",
    "\n",
    "        while True:\n",
    "            w = torch.argmax(residual_vector).item()\n",
    "            if residual_vector[w] <= self.alpha * e_inv:\n",
    "                break\n",
    "            for u in self.incoming_neighbors[w]:\n",
    "\n",
    "                big_d = (1 - self.alpha) * residual_vector[w] / self.out_degrees[u]\n",
    "                estimate_vector[u] += big_d\n",
    "                residual_vector[u] += big_d\n",
    "                if estimate_vector[u] > self.e_rev:\n",
    "                    target_set.add(u)\n",
    "                    frontier_set.update(self.incoming_neighbors[u])\n",
    "            residual_vector[w] = 0\n",
    "\n",
    "        frontier_set = frontier_set - target_set\n",
    "\n",
    "        return target_set, frontier_set, residual_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_nk = nk.readGraph(\"./foodweb-baydry.konect\")\n",
    "n = G_nk.numberOfNodes()\n",
    "is_undir = not G_nk.isDirected()\n",
    "G_pyg = from_networkit(G_nk)[0]\n",
    "G_pyg = EdgeIndex(G_pyg, sparse_size=(n, n), is_undirected=is_undir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_for_alpha(alpha):\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "\n",
    "    delta = 1 / n\n",
    "    print(\"delta:\", delta)\n",
    "    print(\"alpha:\", alpha)\n",
    "\n",
    "    my_ppr = PPR(G_pyg, alpha=alpha, delta=delta)\n",
    "\n",
    "    random.seed(0)\n",
    "    num_node_pairs = 20\n",
    "    num_below_delta = 0\n",
    "    num_non_zero = 0\n",
    "    average_size_target_set = 0\n",
    "    for _ in range(num_node_pairs):\n",
    "        # print(\"----------------\")\n",
    "        s = random.randint(0, n - 1)\n",
    "        t = s\n",
    "        while t == s:\n",
    "            t = random.randint(0, n - 1)\n",
    "\n",
    "        # print(s, t)\n",
    "        score, target_set_size = my_ppr.fast_ppr(s, t)\n",
    "        # print(score, target_set_size)\n",
    "        if score > 0:\n",
    "            num_non_zero += 1\n",
    "        if score < delta:\n",
    "            num_below_delta += 1\n",
    "        average_size_target_set += target_set_size\n",
    "    average_size_target_set /= num_node_pairs\n",
    "\n",
    "    print(\"below delta:\", int(100 * num_below_delta / num_node_pairs ), \"%\")\n",
    "    print(\"non zero:\", int(100 * num_non_zero / num_node_pairs ), \"%\")\n",
    "    print(\"average size target set:\", average_size_target_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "delta: 0.0078125\n",
      "alpha: 0.0001\n",
      "below delta: 100 %\n",
      "non zero: 70 %\n",
      "average size target set: 16.0\n"
     ]
    }
   ],
   "source": [
    "run_for_alpha(0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "delta: 0.0078125\n",
      "alpha: 0.1\n",
      "below delta: 100 %\n",
      "non zero: 65 %\n",
      "average size target set: 16.65\n"
     ]
    }
   ],
   "source": [
    "run_for_alpha(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "delta: 0.0078125\n",
      "alpha: 0.3\n",
      "below delta: 100 %\n",
      "non zero: 65 %\n",
      "average size target set: 16.65\n"
     ]
    }
   ],
   "source": [
    "run_for_alpha(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "delta: 0.0078125\n",
      "alpha: 0.99\n",
      "below delta: 100 %\n",
      "non zero: 35 %\n",
      "average size target set: 16.0\n"
     ]
    }
   ],
   "source": [
    "run_for_alpha(0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that we may have a bug in the code, since during the frontier calculation rarely any node except `t` itself is added to the target set.\n",
    "\n",
    "This is the same for all kinds of values of `alpha`, meaning that the only impact `alpha` currently has is how long the random walks go on, so it is not surprising that with low values of alpha the non zero scores are higher and the running time is longer.\n",
    "\n",
    "The average size of the frontier+target set is between 16 and 17 (again very similar for all values of alpha). It is important to note though, that we added the neighbors of `t` to the frontier set as long as they weren't contained in the target set. This is because otherwise (possibly because of our bug), the frontier set would be empty most of the time.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
